{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIaLhQJWLAD6hxPnfRzCHE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GAKPAHAMGUE/GAKPAHAMGUE.github.io/blob/main/UAP_TKI\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF\n",
        "!pip install pdfminer.six\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxqF9gVmJTOl",
        "outputId": "3ffdd3b8-119c-4e08-a1e8-8eea66df1f49"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.25.2\n",
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20240706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "from concurrent.futures import ThreadPoolExecutor, wait\n",
        "from datetime import date\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import urllib\n",
        "\n",
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pdfminer import high_level"
      ],
      "metadata": {
        "id": "OtCDa49GIaXE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants\n",
        "MAX_RETRIES = 3\n",
        "RETRY_DELAY = 15  # delay in seconds\n",
        "CSV_FOLDER_PATH = '/content/CSV'\n",
        "PDF_FOLDER_PATH = '/content/PDF'\n",
        "\n",
        "def ensure_directory_exists(folder_name):\n",
        "    path = create_path(folder_name)\n",
        "    create_directory_if_not_exists(path)\n",
        "    return path\n",
        "\n",
        "def create_path(folder_name):\n",
        "    return os.path.join(os.getcwd(), folder_name)\n",
        "\n",
        "def create_directory_if_not_exists(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def fetch_html_with_retries(url):\n",
        "    attempt_count = 0\n",
        "    while attempt_count < MAX_RETRIES:\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            return BeautifulSoup(response.text, \"lxml\")\n",
        "        except requests.RequestException:\n",
        "            attempt_count += 1\n",
        "            time.sleep(RETRY_DELAY)\n",
        "    raise Exception(f\"Failed to fetch HTML after {MAX_RETRIES} retries\")\n"
      ],
      "metadata": {
        "id": "xOU9kiVzId8u"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_detail_from_soup(soup, keyword):\n",
        "    try:\n",
        "        keyword_tag = soup.find(lambda tag: tag.name == \"td\" and keyword in tag.text)\n",
        "        if keyword_tag:\n",
        "            next_tag = keyword_tag.find_next()\n",
        "            detail_text = next_tag.get_text().strip()\n",
        "            return detail_text\n",
        "        else:\n",
        "            return \"Tidak ditemukan\"\n",
        "    except AttributeError:\n",
        "        return \"Error ekstraksi\"\n",
        "\n",
        "def get_pdf(file_url, output_directory):\n",
        "    try:\n",
        "        # Fetch the content from the provided URL\n",
        "        response = urllib.request.urlopen(file_url)\n",
        "        filename = response.info().get_filename().replace(\"/\", \" \")\n",
        "\n",
        "        pdf_content = response.read()\n",
        "        file_path = os.path.join(output_directory, filename)\n",
        "\n",
        "        # Save the PDF content\n",
        "        with open(file_path, \"wb\") as pdf_file:\n",
        "            pdf_file.write(pdf_content)\n",
        "\n",
        "        return io.BytesIO(pdf_content), filename\n",
        "    except (urllib.error.URLError, IOError) as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "        return None, None\n"
      ],
      "metadata": {
        "id": "fkuwJ3JXIj05"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    UNWANTED_TEXTS = [\n",
        "        # Predefined unwanted text patterns for cleanup\n",
        "        \"M a h ka m a h A g u n g R e p u blik In d o n esia\\n\",\n",
        "        \"Disclaimer\\n\",\n",
        "        \"Kepaniteraan Mahkamah Agung Republik Indonesia berusaha untuk selalu mencantumkan informasi paling kini dan akurat sebagai bentuk komitmen Mahkamah Agung untuk pelayanan publik, transparansi dan akuntabilitas\\n\",\n",
        "        \"Direktori Putusan Mahkamah Agung Republik Indonesia\"\n",
        "    ]\n",
        "\n",
        "    # Remove extra spaces, newlines, and unwanted text\n",
        "    text = ' '.join(text.replace('\\n', ' ').split())\n",
        "\n",
        "    # Remove unwanted text from PDF\n",
        "    for unwanted_text in UNWANTED_TEXTS:\n",
        "        text = text.replace(unwanted_text, \"\")\n",
        "\n",
        "    return text\n",
        "\n",
        "def extract_and_clean_text(pdf_path):\n",
        "    text_content = \"\"\n",
        "    with fitz.open(pdf_path) as document:\n",
        "        for page in document:\n",
        "            text_content += page.get_text()\n",
        "    return clean_text(text_content)"
      ],
      "metadata": {
        "id": "Z7d4bLrjIl8e"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_data(url, keyword_url):\n",
        "    # Directory paths for CSV and PDF\n",
        "    path_output = ensure_directory_exists(CSV_FOLDER_PATH)\n",
        "    path_pdf = ensure_directory_exists(PDF_FOLDER_PATH)\n",
        "    today = date.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    # Open page and extract details\n",
        "    soup = fetch_html_with_retries(url)\n",
        "    table = soup.find(\"table\", {\"class\": \"table\"})\n",
        "    judul = table.find(\"h2\").text.strip()\n",
        "    table.find(\"h2\").decompose()\n",
        "\n",
        "    # Extract details by keyword\n",
        "    details = {\n",
        "        \"nomor\": extract_detail_from_soup(table, \"Nomor\"),\n",
        "        \"tingkat_proses\": extract_detail_from_soup(table, \"Tingkat Proses\"),\n",
        "        \"klasifikasi\": extract_detail_from_soup(table, \"Klasifikasi\"),\n",
        "        \"kata_kunci\": extract_detail_from_soup(table, \"Kata Kunci\"),\n",
        "        \"tahun\": extract_detail_from_soup(table, \"Tahun\"),\n",
        "        \"tanggal_register\": extract_detail_from_soup(table, \"Tanggal Register\"),\n",
        "        \"lembaga_peradilan\": extract_detail_from_soup(table, \"Lembaga Peradilan\"),\n",
        "        \"jenis_lembaga_peradilan\": extract_detail_from_soup(table, \"Jenis Lembaga Peradilan\"),\n",
        "        \"hakim_ketua\": extract_detail_from_soup(table, \"Hakim Ketua\"),\n",
        "        \"hakim_anggota\": extract_detail_from_soup(table, \"Hakim Anggota\"),\n",
        "        \"panitera\": extract_detail_from_soup(table, \"Panitera\"),\n",
        "        \"amar\": extract_detail_from_soup(table, \"Amar\"),\n",
        "        \"amar_lainnya\": extract_detail_from_soup(table, \"Amar Lainnya\"),\n",
        "        \"catatan_amar\": extract_detail_from_soup(table, \"Catatan Amar\"),\n",
        "        \"tanggal_musyawarah\": extract_detail_from_soup(table, \"Tanggal Musyawarah\"),\n",
        "        \"tanggal_dibacakan\": extract_detail_from_soup(table, \"Tanggal Dibacakan\"),\n",
        "        \"kaidah\": extract_detail_from_soup(table, \"Kaidah\"),\n",
        "        \"abstrak\": extract_detail_from_soup(table, \"Abstrak\")\n",
        "    }\n",
        "        # Handle PDF extraction\n",
        "    link_pdf, text_pdf, file_name_pdf = \"\", \"\", \"\"\n",
        "    try:\n",
        "        link_pdf = soup.find(\"a\", href=re.compile(r\"/pdf/\"))[\"href\"]\n",
        "        file_pdf, file_name_pdf = get_pdf(link_pdf, path_pdf)\n",
        "        text_pdf = high_level.extract_text(file_pdf)\n",
        "        text_pdf = clean_text(text_pdf)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Create DataFrame to hold extracted data\n",
        "    data = [list(details.values()) + [judul, url, link_pdf, file_name_pdf, text_pdf]]\n",
        "    result = pd.DataFrame(\n",
        "        data,\n",
        "        columns=[\n",
        "            \"nomor\", \"tingkat_proses\", \"klasifikasi\", \"kata_kunci\", \"tahun\", \"tanggal_register\",\n",
        "            \"lembaga_peradilan\", \"jenis_lembaga_peradilan\", \"hakim_ketua\", \"hakim_anggota\", \"panitera\",\n",
        "            \"amar\", \"amar_lainnya\", \"catatan_amar\", \"tanggal_musyawarah\", \"tanggal_dibacakan\",\n",
        "            \"kaidah\", \"abstrak\", \"judul\", \"link\", \"link_pdf\", \"file_name_pdf\", \"text_pdf\"\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Save result as CSV\n",
        "    keyword_url = keyword_url.replace(\"/\", \" \")\n",
        "    destination = f\"{path_output}/putusan_ma_{keyword_url}_{today}\"\n",
        "    result.to_csv(f\"{destination}.csv\", index=False, mode='w' if not os.path.isfile(f\"{destination}.csv\") else 'a')\n",
        "\n"
      ],
      "metadata": {
        "id": "CCnNFfePIoQQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_scraper(keyword=None, url=None, sort_date=True, download_pdf=True):\n",
        "    if not keyword and not url:\n",
        "        print(\"Please provide a keyword or URL\")\n",
        "        return\n",
        "\n",
        "    link = f\"https://putusan3.mahkamahagung.go.id/search.html?q={keyword}&page=1\" if not url else url\n",
        "\n",
        "    soup = fetch_html_with_retries(link)\n",
        "\n",
        "    last_page = int(soup.find_all(\"a\", {\"class\": \"page-link\"})[-1].get(\"data-ci-pagination-page\"))\n",
        "\n",
        "    if url:\n",
        "        print(f\"Scraping with url: {url} - {20 * last_page} data - {last_page} page\")\n",
        "    else:\n",
        "        print(f\"Scraping with keyword: {keyword} - {20 * last_page} data - {last_page} page\")\n",
        "\n",
        "    futures = []\n",
        "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        for page in range(last_page):\n",
        "            futures.append(executor.submit(run_process, keyword_url=keyword, page=page+1, sort_date=sort_date))\n",
        "\n",
        "    wait(futures)\n",
        "\n",
        "def run_process(keyword_url, page, sort_date):\n",
        "    link = f\"https://putusan3.mahkamahagung.go.id/search.html?q={keyword_url}&page={page}\" if not keyword_url.startswith(\"https\") else f\"{keyword_url}&page={page}\"\n",
        "    if sort_date:\n",
        "        link += \"&obf=TANGGAL_PUTUS&obm=desc\"\n",
        "\n",
        "    print(link)\n",
        "    soup = fetch_html_with_retries(link)\n",
        "\n",
        "    links = soup.find_all(\"a\", {\"href\": re.compile(\"/direktori/putusan\")})\n",
        "    for link in links:\n",
        "        extract_data(link[\"href\"], keyword_url)\n",
        "\n",
        "def scrape_specific_url(url, download_pdf=True):\n",
        "    if not url or not url.startswith(\"https://\"):\n",
        "        print(\"Please provide a valid URL\")\n",
        "        return\n",
        "\n",
        "    extract_data(url, url)"
      ],
      "metadata": {
        "id": "Cz184IcVI5kT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def open_page(url):\n",
        "    \"\"\"Fetch the web page from the provided URL and return the BeautifulSoup object.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Will raise HTTPError for bad status codes\n",
        "        return BeautifulSoup(response.text, \"lxml\")\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching the page: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "Ev9ddesMUSXJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_scraper(keyword=None, url=None, sort_date=True, download_pdf=True):\n",
        "    if not keyword and not url:\n",
        "        print(\"Please provide a keyword or URL\")\n",
        "        return\n",
        "\n",
        "    path_output = '/content/CSV'\n",
        "    path_pdf = '/content/PDF'\n",
        "\n",
        "    today = date.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    link = f\"https://putusan3.mahkamahagung.go.id/search.html?q={keyword}&page=1\"\n",
        "\n",
        "    if url:\n",
        "        link = url\n",
        "\n",
        "    soup = open_page(link)\n",
        "\n",
        "    last_page = int(\n",
        "        soup.find_all(\"a\", {\"class\": \"page-link\"})[-1].get(\"data-ci-pagination-page\")\n",
        "    )\n",
        "\n",
        "    if url:\n",
        "        print(f\"Scraping with url: {url} - {20 * last_page} data - {last_page} page\")\n",
        "    else:\n",
        "        print(f\"Scraping with keyword: {keyword} - {20 * last_page} data - {last_page} page\")\n",
        "\n",
        "    if url:\n",
        "        keyword_url = url\n",
        "    else:\n",
        "        keyword_url = keyword\n",
        "\n",
        "    futures = []\n",
        "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        for page in range(last_page):\n",
        "            futures.append(\n",
        "                executor.submit(run_process, keyword_url, page + 1, sort_date)\n",
        "            )\n",
        "    wait(futures)\n",
        "\n",
        "def run_process(keyword_url, page, sort_page):\n",
        "    if keyword_url.startswith(\"https\"):\n",
        "        link = f\"{keyword_url}&page={page}\"\n",
        "    else:\n",
        "        link = f\"https://putusan3.mahkamahagung.go.id/search.html?q={keyword_url}&page={page}\"\n",
        "    if sort_page:\n",
        "        link = f\"{link}&obf=TANGGAL_PUTUS&obm=desc\"\n",
        "\n",
        "    print(link)\n",
        "\n",
        "    soup = open_page(link)\n",
        "    links = soup.find_all(\"a\", {\"href\": re.compile(\"/direktori/putusan\")})\n",
        "\n",
        "    for link in links:\n",
        "        extract_data(link[\"href\"], keyword_url)\n",
        "\n",
        "def scrape_specific_url(url, download_pdf=True):\n",
        "    if not url or not url.startswith(\"https://\"):\n",
        "        print(\"Please provide a valid URL\")\n",
        "        return\n",
        "\n",
        "    path_output = '/content/CSV'\n",
        "    path_pdf = '/content/PDF'\n",
        "    today = date.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    extract_data(url, url, path_output, path_pdf, today)"
      ],
      "metadata": {
        "id": "J-339ir7JEh2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Kelas 1A/Kelas 1A Khusus PN Makassar (Perdata Wanprestasi)\n",
        "run_scraper(url=\"https://putusan3.mahkamahagung.go.id/search.html?q=narkotika&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=0&court=097492PN89++++++++++++++++++++++&t_put=&t_reg=&t_upl=&t_pr=\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMMAAi__JJcw",
        "outputId": "1e7199be-7f99-4342-eb9a-2fd8f31cc609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping with url: https://putusan3.mahkamahagung.go.id/search.html?q=narkotika&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=0&court=097492PN89++++++++++++++++++++++&t_put=&t_reg=&t_upl=&t_pr= - 2880 data - 144 page\n",
            "https://putusan3.mahkamahagung.go.id/search.html?q=narkotika&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=0&court=097492PN89++++++++++++++++++++++&t_put=&t_reg=&t_upl=&t_pr=&page=1&obf=TANGGAL_PUTUS&obm=desc\n",
            "https://putusan3.mahkamahagung.go.id/search.html?q=narkotika&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=0&court=097492PN89++++++++++++++++++++++&t_put=&t_reg=&t_upl=&t_pr=&page=2&obf=TANGGAL_PUTUS&obm=desc\n",
            "https://putusan3.mahkamahagung.go.id/search.html?q=narkotika&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=0&court=097492PN89++++++++++++++++++++++&t_put=&t_reg=&t_upl=&t_pr=&page=3&obf=TANGGAL_PUTUS&obm=desc\n",
            "https://putusan3.mahkamahagung.go.id/search.html?q=narkotika&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=0&court=097492PN89++++++++++++++++++++++&t_put=&t_reg=&t_upl=&t_pr=&page=4&obf=TANGGAL_PUTUS&obm=desc\n",
            "https://putusan3.mahkamahagung.go.id/search.html?q=narkotika&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=0&court=097492PN89++++++++++++++++++++++&t_put=&t_reg=&t_upl=&t_pr=&page=5&obf=TANGGAL_PUTUS&obm=desc\n",
            "https://putusan3.mahkamahagung.go.id/search.html?q=narkotika&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=0&court=097492PN89++++++++++++++++++++++&t_put=&t_reg=&t_upl=&t_pr=&page=6&obf=TANGGAL_PUTUS&obm=desc\n",
            "https://putusan3.mahkamahagung.go.id/search.html?q=narkotika&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=0&court=097492PN89++++++++++++++++++++++&t_put=&t_reg=&t_upl=&t_pr=&page=7&obf=TANGGAL_PUTUS&obm=desc\n",
            "https://putusan3.mahkamahagung.go.id/search.html?q=narkotika&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=0&court=097492PN89++++++++++++++++++++++&t_put=&t_reg=&t_upl=&t_pr=&page=8&obf=TANGGAL_PUTUS&obm=desc\n",
            "https://putusan3.mahkamahagung.go.id/search.html?q=narkotika&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=0&court=097492PN89++++++++++++++++++++++&t_put=&t_reg=&t_upl=&t_pr=&page=9&obf=TANGGAL_PUTUS&obm=desc\n",
            "https://putusan3.mahkamahagung.go.id/search.html?q=narkotika&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=0&court=097492PN89++++++++++++++++++++++&t_put=&t_reg=&t_upl=&t_pr=&page=10&obf=TANGGAL_PUTUS&obm=desc\n",
            "https://putusan3.mahkamahagung.go.id/search.html?q=narkotika&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=0&court=097492PN89++++++++++++++++++++++&t_put=&t_reg=&t_upl=&t_pr=&page=11&obf=TANGGAL_PUTUS&obm=desc\n",
            "https://putusan3.mahkamahagung.go.id/search.html?q=narkotika&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=0&court=097492PN89++++++++++++++++++++++&t_put=&t_reg=&t_upl=&t_pr=&page=12&obf=TANGGAL_PUTUS&obm=desc\n",
            "https://putusan3.mahkamahagung.go.id/search.html?q=narkotika&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=0&court=097492PN89++++++++++++++++++++++&t_put=&t_reg=&t_upl=&t_pr=&page=13&obf=TANGGAL_PUTUS&obm=desc\n",
            "https://putusan3.mahkamahagung.go.id/search.html?q=narkotika&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=0&court=097492PN89++++++++++++++++++++++&t_put=&t_reg=&t_upl=&t_pr=&page=14&obf=TANGGAL_PUTUS&obm=desc\n"
          ]
        }
      ]
    }
  ]
}